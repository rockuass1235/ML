{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine(SVM)\n",
    "\n",
    "svm在過去的某一個時代裡曾被稱之為萬用機，它不同於logistic regression沒有區域極值的問題，margin的maxiumize也比單純用L2 norm更能緩解過擬合問題，複雜度也比直接求導來的小，在非線性回歸能力更是打敗polynomial regression，對於高維度處理能力也無人出其左右，在諸神未出的年代裡它就是當之無愧的霸主。\n",
    "\n",
    "當然以現今的角度來看，它的非線性處理能力偏低僅靠映射至高維空間處理，只能處理小數據問題、大數據情況下記憶體與運算會暴增甚至效果不佳，需要調整的超參數不少，對於異常值(標準差、Noise)非常敏感，因此後來被隨機森林演算法取代。\n",
    "\n",
    "\n",
    "## 原理\n",
    "\n",
    "SVM是一種監督式的學習方法，其基礎的概念非常簡單，就是找到一個決策邊界(decision boundary)讓兩類之間的邊界(margins)最大化，使其可以完美區隔開來。\n",
    "\n",
    "一般來說所謂的分類問題就是如何找到一條回歸線將資料分開，假設我們的資料只有2類，我們可能有無限多種的回歸線可以選擇: \n",
    "![](imgs/boundary.jpg)\n",
    "實際上每一條回歸線有好有壞，不好的回歸線可能非常貼近各類資料，以至於當有新的資料加入時會導致分類錯誤，所以我們希望回歸線能離過類資料越遠越好也就是margin越大越好，透過圖片我們可以發現決定margin大小的只有最外部的資料，我們叫這些資料為support vector:\n",
    "\n",
    "![](imgs/margin.jpg)\n",
    "\n",
    "\n",
    "## 演算法\n",
    "\n",
    "有一個二維平面,平面上有兩種不同的數據,分別用圈和叉表示。 由於這些數據是線性可分的,所以可以用一條直線將這兩類數據分開,這條直線就相當於一個超平面,超平面一邊的數據點所對應的y全是-1 ,另一邊所對應的y全是1。\n",
    "\n",
    "![](imgs/ex1.jpg)\n",
    "\n",
    "這個超平面可以用分類函數表示,當f(x) 等於0的時候,x便是位於超平面上的點,而f(x)大於0的點對應 y=1 的數據點,f(x)小於0的點對應y=-1的點,如下圖所示:\n",
    "\n",
    "![](imgs/ex1_margin.jpg)\n",
    "\n",
    "\n",
    "### 設計最大間隔類別器Maximum Margin Classifier\n",
    "\n",
    "對一個數據點進行分類,當超平面離數據點的\"間隔\"越大,分類的確信度(confidence)也越大。 所以,為了使得分類的確信度盡量高,需要讓所選擇的超平面能夠最大化這個「間隔」 值。 這個間隔就是下圖中的Gap的一半。\n",
    "\n",
    "![](imgs/ex1_gap.jpg)\n",
    "\n",
    "OK,到此為止,算是瞭解到了SVM的第一層,對於那些只關心怎麼用SVM的朋友便已足夠,不必再更進一層深究其更深的原理。\n",
    "\n",
    "[資料](https://blog.csdn.net/v_july_v/article/details/7624837)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以sklearn iris datasets建立測試用資料\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "features = iris['data']\n",
    "labels = iris['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147 / 150\n",
      "label:  [False False False False  True False  True False False False]\n",
      "predict:  [False False False False  True False False False False False]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def MinMax(arr):\n",
    "    return (arr-arr.min(axis=0))/(arr.max(axis=0)-arr.min(axis=0))\n",
    "\n",
    "def shuffle(x, y):\n",
    "    \n",
    "    indices = np.arange(0, len(y))\n",
    "    np.random.shuffle(indices)\n",
    "    return x[indices], y[indices]\n",
    "    \n",
    "\n",
    "def acc(yhat, y):\n",
    "    yhat = yhat.reshape(-1)\n",
    "    y = y.reshape(-1)\n",
    "    count = (yhat==y).sum()\n",
    "    print('%d / %d' %(count, len(y)))\n",
    "    \n",
    "train_x = MinMax(features)\n",
    "train_y = labels==1\n",
    "\n",
    "train_x, train_y = shuffle(train_x, train_y)\n",
    "\n",
    "net = SVC(kernel='poly', degree=3, coef0=1, C=5)\n",
    "net.fit(train_x, train_y)\n",
    "\n",
    "\n",
    "\n",
    "acc(net.predict(train_x), train_y)\n",
    "print('label: ', train_y[:10])\n",
    "print('predict: ', net.predict(train_x[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
